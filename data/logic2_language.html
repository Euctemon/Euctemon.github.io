<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <link rel="stylesheet" type="text/css" href="../styles/styles.css">
    <link rel="icon" href="../styles/favicon.ico">
    

    <!-- Katex extension -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

    <script type="text/javascript" src="../styles/personal_scripts.js"></script>
    
    <title>SAT solvers</title>
</head>

<body>

    <header>
        <div class="header-grid">
            <button class="nav-button" onClick="openMenu('mainnav-wrapper','nav-ico')" type="button"><span hidden>Menu</span><img src="../styles/sidebar_open.svg" id="nav-ico"></button>
            <div class="header-name">Logic</div>
        </div>
        
        <div id="mainnav-wrapper">
            <nav>
                <ul>
                    <li><a href="../index.html">Home</a></li>
                    <li class="divider">Chapters</li>
                    <li><a href="SAT1_intro.html">Introduction</a></li>
                    <li><a href="SAT2_syntax.html">Syntax</a></li>
                    <li><a href="SAT3_semantics.html">Semantics</a></li>
                    <li><a href="SAT4_basicsolver.html">Basic solver</a></li>
                </ul>
            
                <ul class="responsive-ul">
                    <li class="divider">This chapter</li>
                    <li><a href="#models">Models</a></li>
                    <li><a href="#soundness">Soundness</a></li>
                    <li><a href="#completeness">Completeness</a></li>
                </ul>
            </nav>
        </div>
    </div>

    </header>
        
    <div class="main-content">
        <article>
            <section id="languages">
                <h2>Languages</h2>

                <p>
                    In the introduction, we noted that logic is concerned with the problem of correct ways of reasoning. Informal logic deals with critical thinking, unravelling fallacies, and constructing sound arguments in natural language. Mathematicians are not brave enough to tackle this problem in its full generality and leave it to philosophers and linguists. They turn their attention to formal logic and formal languages. Such languages have strict rules, which enable unambiguous interpretation. Let's take the law of excluded middle as an example. Philosophers debate whether it should or should not be a law. Mathematicians either involve such a statement in the formal system or not.
                </p>

                <p>
                    One way mathematicians use logic is to express ideas in other mathematical branches such as analysis or algebra. Almost every statement in mathematics involves quantifiers or logical connectives, and mathematicians need to be able to work with such statements. But they also use logic to study the structure of mathematics. For example, one can study the structure, length, or complexity of proofs. In this way mathematicians use logic to study logic. In a broad sense, we are concerned with two things in mathematical logic. The first one is building new things out of old ones. We do this using the concept of a function. The second one is describing connections between chosen things. We do this using the concept of a relation. We start the journey of exploring logic with a definition of a language.
                </p>

                <div class="definition">
                    A <b>language</b> is a triple \(\mathcal{L} = (\Gamma, \Delta, \Alpha) \) where 
                    
                    <ul>
                        <li>\(\Gamma\) is a set of function symbols</li>
                        <li>\(\Delta\) is a set of relation symbols</li>
                        <li>\(\Alpha\) is a function assigning a natural number to every function and relation symbol</li>
                    </ul>
                </div>

                <p>
                    This doesn't look like a language at all. If you ask a linguist, they will shake their head over this definition. The triple \((\Gamma, \Delta, \Alpha)\) contains the so-called non-logical symbols, which we want to include in our theory. It also includes a function \(\Alpha\) called the arity function. Its domain is the set \(\Gamma \cup \Delta\) and its codomain is the set \(\mathbb{N}\). The arity function simply describes the number of symbols the function or relation works with. In this sense, it is a form of a language that we use to describe mathematical structures. Another common name for this triple is a signature, which is perhaps more fitting. For alphabet \(\Sigma\) we use distinct symbols called variables, parentheses, and a comma. This means that given variables \(\alpha,\beta\) and a function symbol \(f\) of arity two, we can construct a word \(f(\alpha,\beta)\). What is the general way of constructing words, or as a mathematician would say, terms?
                </p>

                <div class="definition">
                    Let \(\mathcal{L} = (\Gamma, \Delta, \Alpha)\) be a language. The set of <b>terms</b> of \(\mathcal{L}\) is the smallest set of strings over the alphabet \(\Sigma\) satisfying the following
                    <ul>
                        <li>If \(x\) is a variable, then \(x\) is a term</li>
                        <li>If \(c\) is a constant function symbol, then \(c\) is a term</li>
                        <li>If \(f\) is a function symbol of arity \(n\) and \(t_1,\ldots,t_n\) are all terms, then \(f(t_1,\ldots,t_n)\) is a term</li>
                    </ul>
                </div>

                <p>
                    What are some simple languages? The simplest language has no function or relation symbols at all. We can write such language as \(\mathcal{L}_\varnothing = (\varnothing, \varnothing, \Alpha)\) where \(\varnothing\) is the empty set. If you wonder whether the function \(\Alpha\) is well defined, then don't worry. It's perfectly fine to have a function with the empty set as its domain. This means that the language, or signature, makes sense. What are the terms of \(\mathcal{L}_\varnothing\)? The only way to build terms in this language is from variables. This means that terms of \(\mathcal{L}_\varnothing\) over a given alphabet \(\Sigma\) are exactly the variables of \(\Sigma\). If the alphabet does not contain any variables, then there are no terms.
                </p>

                <p>
                    Let's make the language a little more interesting. We will write \(\mathcal{L}_1\) for a language where the only function symbol is \(f\) with arity one and the only relation symbol is \(r\) with arity two. In this language the arity function is defined as \(\Alpha(f) = 1\) and \(\Alpha(r) = 2\). To talk about terms, we need to specify the alphabet. For example, the alphabet \(\Sigma_1\) containing only one variable \(\alpha\) is a nice alphabet. Having specified language and alphabet, we can start writing terms. For example, \(\alpha\) or \(f(f(\alpha))\) are both terms of this language. If we abbreviate \(f ( \cdots f(\alpha))\) where we applied \(f\) exactly \(n\) times as \(f^n(\alpha)\), then it is not hard to see that every term has the form \(f^n(\alpha)\) for some natural number \(n\). In the case of zero, we say that \(f^0(\alpha)\) means \(\alpha\).
                    
                <h2>Model theory</h2>
                    We now get to relations. These are trickier than functions. A function can be thought of as a recipe for modifying old things or creating new things out of old ones. For example squaring a number is a function. Another example might be a cooking recipe, where the input are the ingredients and the output is a sweet pie of your choosing. On the other hand relations express a connection between things. For example two people might be cousing are colleagues. In both examples the two people are in some kind of relation(ship). In logic we use relations 
                </p>

                <div class="definition">
                    Let \(\mathcal{L} = (\Gamma, \Delta, \Alpha)\) be a language. The set of <b>atomic formulas</b> of \(\mathcal{L}\) is the smallest set of strings over the alphabet \(\Sigma\) satisfying the following
                    <ul>
                        <li>If \(r\) is a relation symbol of arity \(n\) and \(t_1,\ldots,t_n\) are all terms, then \(f(t_1,\ldots,t_n)\) is an atomic formula</li>
                    </ul>
                </div>

                <p>
                    The full definition of a formula with logical symbols will come later on when we learn more about Model theory. But even this simple definition offers a great deal of discussion. First we get aquainted with formulas in our language \(\mathcal{L}_1\). The alphabet is still the same, namely \(\Sigma_1\). We already know that terms in \(\mathcal{L}_1\) look like \(f^n(\alpha)\). We also know that the relation symbol \(r\) has the arity of two. Putting it all together, we see that all formulas have the form \(r(f^n(\alpha),f^m(\alpha))\). Note that if we change the alphabet, we can get a very different set of terms and atomic formulas.
                </p>

                
                <p>
                    In the introduction, we said that relations should model the connections between things we are interested in. All we have now are just some abstract symbols. In other words, we need to interpret the function and relation symbols to something more concrete and tangible. Dealing with interpreting signatures is the business of Model theory.
                </p>

                <div class="definition">
                Fix a language \(\mathcal{L} = (\Gamma,\Delta,\Alpha)\). We say that \(\mathcal{M} = (U,I)\) is a <b>model</b> for language \(\mathcal{L}\) if \(U\) is a nonempty set and \(I\) is a function which assigns to each k-ary function symbol in \(\Gamma\) a k-ary function from \(U\) to \(U\) and to each k-ary relation symbol in \(\Delta\) a k-ary relation on \(U\). We call the set \(U\) a universe.
                </div>

                <p>
                    So how do relations represent connections between things? Imagine you have 
                    
                    
                    It's time to think about relations. Mathematicians view relations as subsets of cartesian products of sets. If we take a set \(\{\bullet, \circ\}\), then a relation of arity two is a subset of the set \(\mathcal{X} = \{\{\bullet,\bullet\},\{\bullet,\circ\},\{\circ,\bullet\},\{\circ,\circ\}\}\). For example the set \(\mathcal{R} = \{\{\bullet,\circ\},\{\circ,\bullet\},\}\) is a valid relation. We can turn a relation into a function as follows - for every set \(x\) in the set \(\mathcal{X}\) we write \(f_r(x) = \bold{1}\) if \(x\) is in the set \(\mathcal{R}\) and \(\bold{0}\) otherwise. We will see later that we want to distiguish between functions and relations when we say what it means for a formula to be <i>true</i>.
                </p>

                <p>
                    Let's take the universe \(U = \{\bullet, \circ\} \) and the assignment function \(\mathcal{I}\) where \(f^\mathcal{I} (\bullet) \to \circ\), \(f^\mathcal{I} (\circ) \to \bullet\) and \(g^\mathcal{I}\) is given by \(\mathcal{R}\) from the previous paragraph. We constructed our first model \(\mathcal{M} = (U,\mathcal{I})\).
                    
                    
                    How many models the language \(L\) admit for the universe \(U\)? The function \(f\) has the arity of one, which means we want to count the number of bijections \(U \to U\). This is 2. The relation has the arity of two meaning we want to count the number of subsets of \(U \times U\). That is 4.  We can combines the interpretations of function \(f\) and relation \(r\) freely, which means \(L\) has 8 different models on \(U\).
                </p>
                
                <div class="definition">
                    If a formula \(\varphi\) is true in all models of the language \(L\), we say that \(\varphi\) is tautology.
                </div>



                Afirst-order theory or simply a theory T consists of a first-order language L(T)
                or simply L andasetof L-formulas,calledtheaxiomsof T.Foracardinalκ ≥ℵ0, T
                is called a κ-theory if |L(T)|≤κ. ℵ0-theories are simply called countable theories.
                Amodel of T is an L-structure M in which each axiom of T is true. If M is a model
                of T,weshallwriteM |= T.Aformulaϕ is called a theorem of T, written T |= ϕ,
                if it is true in all models of T.







                --- no no no ---
                <p>
                    These definitions are rather abstract so it calls for a concrete example. Suppose we have a language with one function symbol \(c\) of zero arity, often call a constant, two binary function symbols \(f,g\) and one binary relation \(r\). We specified the arity of all symbols and the sets of functional symbols and relational symbols are disjoint. This means we have a language \(L\) written as \((\{c,f,g\},\{r\},a)\) where \(a(c) = 0\), \(a(f) = 2\), \(a(g) = 2\) and \(a(r) = 2\). One such model for this language are the natural numbers. We assign \(0\) to the constant \(c\), the functions \(+\) and \(\times\) for function symbols \(f\) and \(g\) and finally the relation \(\leq\) for relation symbol \(r\). The universe is simply the set of natural numbers. We can write this model \(\mathfrak{M}\) as \( (\mathbb{N},\{0,+, \times, \leq\}) \). Note that the functions and the relation satisfy the the conditions on arity.
                </p>

                <h3>Propositional logic</h3>

                <p>
                    It's important to know that propositional logic is a <i>language</i>. This means we need to specify its function and relation symbols and their arities. This logic has one functional symbol of zero arity, namely the symbol for falsity written as \(\bot\). It has three binary functional symbols \(\land,\lor\) and \(\to\) and no relation symbols. We call the set \(\bot,\land,\lor,\to\) an alphabet. We also use a set of <i>propositional variables</i> such as \(P,Q\) or \(R\). Having fixed the alphabet we move to defining the set of propositional formulas.
                </p>

                <div class="definition" id="alphabet">
                The set of propositional formulas is a smallest set of strings over the alphabet \( (\bot,\land,\lor,\to) \) satisfying
                <ul>
                    <li>If \(P\) is a propositional variable, then \(P\) is a propositional formula</li>
                    <li>\(\bot\) is a propositional formula</li>
                    <li>If \(A\) and \(B\) are propositional formulas, then \(A \land B\), \(A \lor B\) and \(A \rightarrow B\) are propositional formulas.</li>
                </ul>
                </div>

                <p>
                    You might wonder why we did not include the propositional variables in the alphabet. The ideal is that the language of propositional logic is just fine without any more symbols. We <i>could</i> reason only about formulas without variables such as \(\bot \to (\bot \land (\bot \lor \bot))\). But we want to have more freedom. We want to reason about propositions such as \(\bot \to (\bot \land A)\) and then substitute any proposition for \(A\). One pillar of reasoning in classical propositional logic is that from falsity, everything follows. We express this idea as the proposition \(\bot \to A\) being always true. If we had not propositional variables, we would need to write this rule for every valid string of alphabet such as \(\bot \to (\bot \land \bot)\) or \(\bot \to ((\bot \lor \bot) \land \bot)\) and so on for infinitely many possible strings. That cannot be done with sanity. People who want to be precise or annoing would say that using propositional variable, we work in a <i>meta language</i>, or one level of abstraction higher. The same problem arises when we talk about proofs or even prove theorems about proofs.
                </p>

                <p>
                    The alphabet used in the definition of propositional formulas is in some sense complete. But we also use some other symbols as \(\top, \lnot\) or \(\leftrightarrow\) for more clarity. These are just abbreviations. The truth value \(\top\) is an abbreviation of \(\bot \to \bot\), the negation \(\lnot A\) is abbreviation of \(A \to \bot\) and bidirectional implication \(A \leftrightarrow B\) is an abbreviation of \(A \to B \land B \to A\).
                </p>

            </section>

            <section id="deductive">
                <h2>Deductive systems</h2>
                <h3>Axiomatic systems</h3>

                <p>
                    So far we know what a language is and we know how construct the set of propositions over a given alphabet. In order to give a meaning to the propositions we need rules for manipulating them. Dealing with languages and rules is the syntactic part of logic. One of the earliest logical systems are the axiomatic systems, also called Hilbert style systems. These systems consist of a set of axioms and inference rules. For example the axiomatic system for <i>classical propositional logic</i> consists of nine <i>axiom schemas</i> and one inference rule, namely the <i>modus ponens</i>. The axiom schemas are called schemas because the act as blueprints for the actual rules. We can take the first axiom schema a instantiate it with \(P \to P\) for \(A\) and \(\bot\) for \(B\). The concrete rule is then the proposition \((P \to P) \to (\bot \to (P \to P))\) and we take this proposition to be true.
                </p>
                
                <ol>
                    <li>\(A \to (B \to A)\)</li>
                    <li>\( (A \to (B \to C)) \to ((A \to B) \to (A \to C)) \)</li>
                    <li>\(A \to (B \to A \land B)\)</li>
                    <li>\(A \land B \to A\)</li>
                    <li>\(A \land B \to B\)</li>
                    <li>\(A \to (B \lor A)\)</li>
                    <li>\(B \to (B \lor A)\)</li>
                    <li>\( (A \to C) \to (B \to C) \to (A \lor B \to C) \)</li>
                    <li>\(\lnot \lnot A \to A\)</li>
                </ol>
                
                <p>
                    The modus ponens is the only rule with which we can build new propositions out of old ones. This rule acts on implication. Let's say we can prove \(A\) and \(A \to B\). Using modus ponens we can conclude that also \(B\) holds. It's time for a proof of the commutativity of conjunction.
                </p>

                <div class="theorem">
                    If \(P \land Q\) holds, then \(Q \land P\) holds.
                </div>

                <div class="proof">
                    <ol>
                        <li>Assume that \(P \land Q\) holds.</li>
                        <li>Obtain \(P\) by modus ponens using assumption and fourth axiom schema.</li>
                        <li>Obtain \(Q\) by modus ponens using assumption and fifth axiom schema.</li>
                        <li>Obtain \(P \to (Q \land P)\) by modus ponens using \(Q\) and third axiom schema.</li>
                        <li>Obtain \(Q \land P\) by modus ponens using \(P \to (Q \land P)\) and \(P\).</li>
                    </ol>
                    \(\blacksquare\)
                </div>

                <p>
                    How can we describe what a proof is? Informally a proof is what we just did. More formally, it is a sequence \(\Psi_0,\ldots,\Psi_n\) of propositions such that for every \(k < n + 1\), either \(\Psi_k\) is a proposition in context \(\Gamma\), or it is an axiom schema or there are indices \(i< k\) and \(j < k\) such that \(\Psi_k\) follow from \(\Psi_i\) and \(\Psi_j\) by modus ponens. We then say that \(\Psi_n\) is the conclusion and the sequence \(\Psi_0,\ldots,\Psi_n\) with hypotheses in \(\Gamma\) is a proof of \(\Psi_n\).
                </p>

                <p>
                    Couple of remarks about the assumptions. The assumtions we make is a set called context and we write \(\Gamma \vdash A\) to say that \(A\) is provable in the context of \(\Gamma\). This means we can write the last theorem as \(\{P \land Q\} \vdash Q \land P\).
                    
                    
                    Some statemens are provable without use of any concrete assumption. For example concrete instances of axiom schemas do not need any context, as they are axioms and the corresponding context is the empty set. Commonly used symbol for empty is \(\varnothing\). This means we can write \(\varnothing \vdash P \land Q \to P\) as an instance of the third axiom schema. Can we obtain anything else then axioms from an empty context? We can. One such example is the proposition \(P \to P\).
                </p>

                <div class="theorem">
                    We have \(\varnothing \vdash P \to P\).
                </div>

                <div class="proof">
                    <ol>
                        <li>Obtain \(P \to (P \to P)\) as an instance of the first axiom schema.</li>
                        <li>Obtain \(P \to ((P \to P) \to P)\) as an instance of the first axiom schema.</li>
                        <li>Obtain \((P \to ((P \to P) \to P)) \to ((P \to (P \to P)) \to (P \to P))\) as an instance of the second axiom schema.</li>
                        <li>Obtain \(((P \to (P \to P)) \to P \to P) \) by modus ponens using 2 and 3.</li>
                        <li>Obtain \(P \to P \) by modus ponens using 4 and 1.</li>
                    </ol>
                    \(\blacksquare\)
                </div>

                <p>
                    Why isn't this an axiom? It seems useful to know it from the start. We <i>could</i> add \(P \to P\) to the axiom schema and nothing would break. But we strive to have a set of axioms such that this set is in some sense minimal. This means that no axiom in the set is provable using the other axioms. The next theorem is useful for proving properties of the set of propositions. It can be rigorously proved but we will not bother with it. It follows from the definition of a proof.
                </p>
                    
                <div class="theorem">
                    Suppose \(\Gamma\) is a set of propositional formulas. The set of formulas (set of consequences) provable from \(\Gamma\) is inductively generated by these rules
                    <ul>
                        <li>If \(A \in \Gamma\), then \(\Gamma \vdash A\)</li>
                        <li>If \(A\) is an axiom schema, then \(\Gamma \vdash A\)</li>
                        <li>If \(\Gamma \vdash A\) and \(\Gamma \vdash A \to B\), then \(\Gamma \vdash B\)</li>
                    </ul>
                </div>

                <p>
                    It is much more important to understand that this theorem subtly hints on the fact that we can use a <i>induction principle</i> for proving theorems about the set of propositions. We will se the induction principle in action when we prove the next theorem. it is called the <b>Deduction theorem</b> and it says something about the interplay of context \(\Gamma\) and implication.
                </p>
                
                <div class="theorem">
                    Fix a context \(\Gamma\) and a proposition \(P\). If \(\Gamma \cup\{P\} \vdash Q\) then \(\Gamma \vdash P \to Q\).
                </div>

                <div class="proof">
                    We fix \(\Gamma\) and \(P\) and use the induction on the set of consequences of \(\Gamma \cup\{P\}\). This means that if \(Q\) is in the set of consequences, it has the form of one of the three cases mentioned earlier.

                <ol>
                    <li>\(Q \in \Gamma, P\)</li>
                    <li>\(Q\) as an axiom schema</li>
                    <li>There is proposition \(C\) such that \(\Gamma \cup\{P\} \vdash C\) and also \(\Gamma \cup\{P\} \vdash C \to Q\)</li>
                </ol>
                
                <p>
                    We start with the first case. This is a tricky one. If \(P \in \Gamma\), we know that \(\Gamma \vdash P\). Using the first axiom schema we also know that \(\Gamma \vdash Q \to (P \to Q)\) as axioms follow from any context! The conclusion \(\Gamma \vdash Q \to P\) is obtained by modus ponens. What if \(Q\) is \(P\) itself? We need to prove that \(\Gamma \vdash P \to P\). Luckily we proved this already. The second case is easier. If \(Q\) is an axiom schema, we know that \(\Gamma \vdash Q\). We again use the first axiom schema to conclude \(\Gamma \vdash Q \to (P \to Q)\) and modus ponens to obtain the conclusion \(\Gamma \vdash P \to Q\).
                </p>

                <p>
                    From the structure of the set of proposition, we know that if \(Q\) is not an axiom schema or is not the element of \(\Gamma \cup\{P\}\), it had to be <i>constructed</i> using modus ponens. We therefore know there has to be a proposition \(C\) satisfying \(\Gamma \cup\{P\} \vdash C\) and also \(\Gamma \cup\{P\} \vdash C \to Q\). Now we finally use the induction. Thi means we can use the conclusion of the theorem for the proposition \(C\). We know that \(\Gamma \cup\{P\} \vdash C\) and thus conclude \(\Gamma \vdash P \to C\). In the same manner we conclude \(\Gamma \vdash P \to (C \to Q)\). We also have the second axiom schema at our disposal in the form \(\Gamma \vdash P \to (C \to Q) \to ((P \to C) \to (P \to Q))\). Again, we know we can prove this as axioms follow from any context. But now it suffices to use modus ponens two times and we are done.
                </p>
                \(\blacksquare\)
                </div>

                <h3>Natural deduction</h3>

                <p>
                    The Hilbert style is not the only way to set up a logic system. The axiomatic systems have mainly axioms and only one rule. What if we had only one axiom and many rules? It turns out that such a system exists. The idea is to reason about the alphabet and not about whole propositions. Let's look at the conjunction. The Hilbert system had an axiom which <i>introduces</i> the proposition involving conjunction, namely the third axiom \(A \to (B \to (A \land B))\). If, for some context \(\Gamma\), we know that \(\Gamma \vdash A\) and \(\Gamma \vdash B\), we can use modus ponens two times to obtain \(\Gamma \vdash A \land B\). It would be nice to have some kind of rule which would build the conjunction out of its parts, something like
                </p>

                $$\dfrac{\Gamma \vdash A \qquad \Gamma \vdash B}{\Gamma \vdash A \land B}$$

                <p>
                    The part above the line describes what we assume and the lower part describes what we can build out of the assumptions. This is in agreement with the axiom schema where the modus ponens is build into the introduction rule itself. We can also break apart the conjuction. This is what the fourth and fifth axiom schemas tell us. In order to completely describe the behaviour of conjunction, we need one introduction rule and two elimination rules.
                </p>

                $$\dfrac{\Gamma \vdash P \qquad \Gamma \vdash Q}{\Gamma \vdash P \land Q} \land_I \qquad \dfrac{\Gamma \vdash P \land Q}{\Gamma \vdash P} \land_E \qquad \frac{\Gamma \vdash P \land Q}{\Gamma \vdash Q} \land_E$$

                <p>
                    The implication has one introductory rule and one elimination rule. The elimination rule is exactly modus ponens. If you had not seen the Deduction theorem, the introduction rule might seem suprising. But when you think about the Deduction theorem, it really <i>is</i> a way to introduce implication. Note how the context is changed from \(\Gamma,P\) to \(\Gamma\).
                </p>

                $$\dfrac{\Gamma \cup\{P\} \vdash Q}{\Gamma \vdash P \to Q} \to_I \qquad \frac{\Gamma \vdash P \to Q \qquad \Gamma \vdash P}{\Gamma \vdash Q} \to_E$$

                <p>
                    Next logical connective is the disjunction. The two introduction rules are modelled according to the sixth and seventh axiom. The elimination rule mirrors the eight axiom and is also a context-changing rule.
                </p>

                $$\dfrac{\Gamma \vdash P}{\Gamma \vdash P \lor Q} \lor_I \qquad \dfrac{\Gamma \vdash Q}{\Gamma \vdash P \lor Q} \lor_I \qquad \dfrac{\Gamma \vdash P \lor Q \qquad \Gamma \cup\{P\} \vdash R \qquad \Gamma \cup\{Q\} \vdash R}{\Gamma \vdash R} \lor_E$$

                <p>
                    The next rule mirrors the ninth axiom schema and is called reductio ad absurdum. after introducing all rules for natural deduction, we will say something more about this rule.
                </p>

                $$\frac{\Gamma \cup\{\lnot P\} \vdash \bot}{\Gamma \vdash P} \bot_E$$

                <p>
                    The last rule is called the assumption rule. Remember that we proved \(P \to P\) from the first two axiom schemas. There is nothing above the line which means we do no thave to assume anything to use this rule, <i>this</i> is the rule for introducing assumptions.
                </p>

                $$\frac{}{\Gamma,P \vdash P}$$

                <p>
                    It seems that we encoded all the axiom schemas into introduction or elimination rules but the first two. Our first task will be to construct a proof of the first axiom in the style of natural deduction. First we use the assumption rule with context \(\Gamma \cup\{B\}\) and proposition \(A\).
                </p>

                $$\dfrac{}{\Gamma \cup\{A,B\} \vdash A}$$

                <p>
                    Using the introduction rule for implication twice, we recover the first axiom schema!
                </p>

                $$\dfrac{\dfrac{\dfrac{}{\Gamma \cup\{A,B\} \vdash A}}{\Gamma \cup\{A\} \vdash B \to A} \to_I}{\Gamma \vdash A \to (B \to A)} \to_I$$

                <p>
                    We just wrote our first proof using in the natural deduction system. We could also recover the second rule. We already know what is meant by a proof in axiomatic system. The notion of proof in natural deduction is not much different. The pair \((\Gamma,A)\) is called a <i>sequent</i> where \(\Gamma\) is a finite set of propositions and we write \(\Gamma \vdash A\). A proof is then a sequence of sequents in which every sequent is obtain from the previous using elimination or introduction rule, apart from the assumption rule.
                </p>

                <p>
                    For the axiomatic system we had a theorem about the nature of provable proposition from the context \(\Gamma\). The next definition describes a similar thing for natural deduction. Thanks to this definition we again obtain the power of induction.
                </p>
                
                <div class="definition">
                    The set of sequents provable in natural deduction is defined as a smallest set of sequents closed under the natural deduction rules.
                </div>

                <p>
                    One such use of induction in natural deduction system is the following lemma called <b>Weakening</b>. It expresses that the natural deduction is a monotonne system. In other words, adding a hypothesis does not destroy proposition which were earlier provable.
                </p>

                <div class="theorem">
                    If \(\Gamma \vdash A\) is provable in natural deduction and \(\Gamma \subset \Delta\), then \(\Delta \vdash A\) is provable in natural deduction.
                </div>

                <div class="proof">
                    We will use induction on the set of provable sequents. But what does that mean? First we deal with the assumption rule. Suppose we know that \(\Gamma \cup\{A\} \vdash A\) is provable and want to prove \(\Delta \cup\{A\} \vdash A\). But we know that \(\Delta \cup\{A\} \vdash A\) holds from the assumption rule alone. What about the other rules? Let's take for example the introduction rule for conjunction.

                    $$\dfrac{\Gamma \vdash P \qquad \Gamma \vdash Q}{\Gamma \vdash P \land Q} \land_I$$

                <p>
                Let's say that \(\Gamma \vdash A \land B\) is provable. This requires that \(\Gamma \vdash A\) and \(\Gamma \vdash B\) are provable too. The induction principle tells us that we can use the theorem statement for the upper part of the introduciton or elimination rule. For the case of introduction rule for conjunction, this means we assume that \(\Delta \vdash A\) and \(\Delta \vdash B\) are provable. using the introduction rule we conclude that also \(\Delta \vdash A \land B\) is provable. Other rules would be checked in the same manner and we would find that all of them respect the provability relation. This means we proved the theorem.
                if \(\Gamma \vdash A\), then \(\Delta \vdash A\).
                </p>
            
                \(\blacksquare\)
                </div>

                <p>
                    This is not a rule as the introduction or elimination rules we have seen so far. We cannot derive this rule using other rules. It is called admissible rule which means it does not change the set of provable sequents. We could work without it, but is pleasant to use. For example we can have a more general form of rules we introduced so far
                </p>

                $$\dfrac{\Gamma \vdash A \qquad \Delta \vdash B}{\Gamma \cup \Delta \vdash A \land B}$$

                <p>
                    You might have been bothered by the form of the elimination rule for disjunction. The axiom schema has the form of explicit implications, \( (A \to C) \to (B \to C) \to (A \lor B \to C) \), but rule is written without implications. The question is, are these two rules equivalent?
                </p>

                $$\dfrac{\Gamma \vdash P \lor Q \qquad \Gamma \cup\{P\} \vdash R \qquad \Gamma \cup\{Q\} \vdash R}{\Gamma \vdash R} \lor_E$$

                $$\dfrac{\Gamma \vdash P \lor Q \qquad \color{red}{\Gamma \vdash P \to R \qquad \Gamma \vdash Q \to R}}{\Gamma \vdash R} \lor_E$$


                <p>
                    Let's say we know, that \(\Gamma \vdash P \lor Q\) and \(\Gamma \cup\{P\} \vdash R\) and \(\Gamma \cup\{Q\} \vdash R\) are provable. Using introduction rule for implication we obtain \(\Gamma \vdash P \to R\) and \(\Gamma \vdash Q \to R\). We can use the second form to conclude that \(\Gamma \vdash R\). What about the other way? Assume we know that \(\Gamma \vdash P \lor Q\) and \(\Gamma \vdash P \to R\) and \(\Gamma \vdash Q \to R\) are provable. How can we use the first form of the rule? We use the weakening property to obtain \(\Gamma \cup\{P\} \vdash P \to R\) and \(\Gamma \cup\{Q\} \vdash Q \to R\). From the assumption rule we know that \(\Gamma \cup\{P\} \vdash P\) and \(\Gamma \cup\{Q\} \vdash Q\). Using introduction rule for implication we finally obtain \(\Gamma \cup\{P\} \vdash R\) and \(\Gamma \cup\{Q\} \vdash R\) and we can use the first form of the rule to conclude that \(\Gamma \vdash R\). This means that the weakening theorem tells us these rules are equivalent! One last remark. We could write the weakening as a rule <i>operating on context</i>
                </p>

                $$\dfrac{\Gamma \vdash P}{\Gamma \cup\{A\} \vdash P} w_I$$

                <H3>Proof by contradiction</H3>

                <p>
                    It might come as a surprise that <i>propositional logic</i> has few subtypes. The most known is the classical propositional logic. Such logic is described by the nine axiom schemas we saw earlier, or the corresponding natural deduction rules. Taking only the first eight axiom schemas would mean we work in <i>minimal propositional logic</i> and replacing the last axiom schema with \(\bot \to A\) would mean we work in <i>intuitionistic propositional logic</i>. The axiom schema \(\lnot \lnot A \to A\) is called the double negation elimination. First task is to recover the double negation elimination axiom from the elimination rule for \(\bot\). We can happily prove falsity using implication elimination rule from two simple assumptions.
                </p>

                $$\dfrac{\dfrac{}{\{\lnot A\} \vdash \lnot A} \qquad \dfrac{}{\{\lnot \lnot A\} \vdash \lnot \lnot A}}{\{\lnot A, \lnot \lnot A\} \vdash \bot} \to_I$$

                <p>
                    Note the lines above the assumptions. This means we needed to use the assumption rule. The second step is to use the elimination rule for \(\bot\) called reduction ad absurdum. After that we use the introduciton rule for implication and we are done.
                </p>

                $$\dfrac{\dfrac{}{\{\lnot A\} \vdash \lnot A} \qquad \dfrac{}{\{\lnot \lnot A\} \vdash \lnot \lnot A}}{\dfrac{\{\lnot A, \lnot \lnot A\} \vdash \bot}{\dfrac{\{\lnot \lnot A\} \vdash A}{\varnothing \vdash \lnot \lnot A \to A} \to_I} \bot_E} \to_E$$

                <p>
                    The reverse implication is similar, but we do not have to use the elimination rule for \(\bot\). This has the consuquence that the reverse implication is provable in minimal propositional logic. We again start with proving falsity.
                </p>

                $$\dfrac{\dfrac{}{\{A\} \vdash A} \qquad \dfrac{}{\{\lnot A\} \vdash \lnot A}}{\{A, \lnot A\} \vdash \bot} \to_E$$

                <p>
                    Two introduction rules for implications are enough to conclude the desired proposition. In classical logic we therefore have the equivalence \(A \leftrightarrow \lnot \lnot A\).
                </p>

                $$\dfrac{\dfrac{}{\{A\} \vdash A} \qquad \dfrac{}{\{\lnot A\} \vdash \lnot A}}{\dfrac{\{A, \lnot A\} \vdash \bot}{\dfrac{\{A\} \vdash \lnot A \to \bot}{\varnothing \vdash A \to \lnot \lnot A} \to_I} \to_I} \to_E$$


                <h3>The law of excluded middle</h3>

                <p>
                    Another famous thing in logic is the law of excluded middle. It says that the proposition \(P \lor \lnot P\) is true without any further assumptions. The proof is tricky and not intuitive at all. Let's start at the end. In order to prove \(P \lor \lnot P\) we use reductio ad absurdum.
                </p>

                $$\frac{\varnothing\vdash \lnot (A \lor \lnot A) \vdash \bot}{\varnothing \vdash A \lor \lnot A} \bot_E$$

                <p>
                    After that we use modus ponens (elimination rule for implication). The right side can be proved by the assumption rule immediately. The problem is with the left side colored in red.
                </p>

                $$\dfrac{\dfrac{\vdots}{\textcolor{red}{\{\lnot(A \lor \lnot A)\} \vdash A \lor \lnot A}} \qquad \dfrac{}{\{\lnot(A \lor \lnot A)\} \vdash\lnot(A \lor \lnot A)}}{\dfrac{\varnothing \vdash \lnot (A \lor \lnot A) \vdash \bot}{\varnothing \vdash A \lor \lnot A} \bot_E} \to_E$$

                <p>
                    We only need to prove one part of the disjunction thanks to the introduction rule for disjunction. If you try to come up with the proof, you would have to try which part is provable. I will reveal that choosing to prove \(\lnot A\) is the right way. The next step is to use reductio ad absurdum again and modus ponens. The left part follows from the assumption rule.
                </p>

                $$\dfrac{\dfrac{}{\{\lnot(A \lor \lnot A)\} \vdash \lnot(A \lor \lnot A)} \qquad \dfrac{\vdots}{\color{red}{\{\lnot \lnot A\} \vdash A \lor \lnot A}}}{\dfrac{\{\lnot \lnot A,\lnot(A \lor \lnot A)\} \vdash \bot}{\dfrac{\{\lnot(A \lor \lnot A)\} \vdash \lnot A}{\color{red}{\{\lnot(A \lor \lnot A)\} \vdash A \lor \lnot A}} \lor_I} \bot_E} \to_E$$

                <p>
                    The right part seems even worse than what we have to prove but don't worry. Knowing \(\lnot \lnot A \vdash A\) we can use introduction rule for disjunction to quickly finish the proof.
                </p>
                
                $$\dfrac{\dfrac{}{\{\lnot \lnot A\} \vdash A} \vdash}{\color{red}{\{\lnot \lnot A\} \vdash A \lor \lnot A}} \lor_I$$

                <p>
                    The provability of \(\{\lnot \lnot A\} \vdash A\) does not follow from the assumption rule but had to be proved. That is why I added the symbol \(\vdash\) next to the line. It is not an offical rule but it is quite reasonable to write it this way. Remember that the symbol \(\vdash\) stands for provability. It makes sense to have a kind of introduction rule for statements <i>we have already proved</i>. We do this in mathematics all the time. We start by proving small lemmas and then use them to make bigger ones until we end up with rather complex theorems. It resembles the <i>structural rule</i> for weakening as we act on the structure of the proof. Of course we could simply copy the whole proof of \(\{\lnot \lnot A\} \vdash A\) here.
                </p>

                
            </section>
        </article>
    </div>
</body>